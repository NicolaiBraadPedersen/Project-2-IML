library(data.table)
library(ggplot2)
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(OpenML)
library(mlr3pipelines)
library(mlr3filters)
library(tidyr)
library(dplyr)
library(glex)
library(xgboost)
library(patchwork)
library(DALEX)
library(DALEXtra)


set.seed(732)

future::plan("multisession")
data = fread("Motor vehicle insurance data.csv")

cond_1 = which(data$Distribution_channel != 1 & data$Distribution_channel != 0)
data$Distribution_channel[cond_1] = matrix(data= NA, nrow=length(cond_1))

#Initialize data
data$ID = as.factor(data$ID)
data$Distribution_channel = as.numeric(data$Distribution_channel)
data$Payment = as.numeric(data$Payment)
data$Type_risk = as.factor(data$Type_risk)
data$Area = as.numeric(data$Area)
data$Second_driver = as.numeric(data$Second_driver)
data$N_doors = as.factor(data$N_doors)
data$Type_fuel = as.factor(data$Type_fuel)

data$Date_start_contract = as.Date(data$Date_start_contract, format = "%d/%m/%Y")
data$Date_start_contract = as.numeric(data$Date_start_contract)
data$Date_birth = as.Date(data$Date_birth, format = "%d/%m/%Y")
data$Date_birth = as.numeric(data$Date_birth)
data$Date_driving_licence = as.Date(data$Date_driving_licence, format = "%d/%m/%Y")
data$Date_driving_licence = as.numeric(data$Date_driving_licence)


data_num = data[,lapply(.SD, mean, na.rm = TRUE), by = ID, .SDcols = is.numeric]
data_num = data_num[,-c("Lapse", "Premium", "N_claims_year", "N_claims_history","R_Claims_history")]
data_fac = data[,lapply(.SD, function(x) x[1]), by = ID, .SDcols = is.factor]
data = merge(data_num, data_fac[,-1], by = "ID")[,-1]
rm(data_fac,data_num,cond_1)

graph = po("imputemean", affect_columns = selector_name("Length")) %>>%  
  po("imputesample", affect_columns = selector_type("factor")) %>>% 
  po("encode") %>>% 
  po("scale")

task_tweedie = as_task_regr(data, target="Cost_claims_year")

lrn_featureless = as_learner(lrn("regr.featureless"))

lrn_tweedie = as_learner(graph %>>% lrn("regr.xgboost", objective="reg:tweedie", max_depth=2, 
                                        nrounds=to_tune(100,500), 
                                        eta = to_tune(0.1,0.3), 
                                        tweedie_variance_power = to_tune(1,2)) )

at_tweedie = auto_tuner(tuner = tnr("random_search", batch_size = 8),
                        learner = lrn_tweedie,
                        resampling = rsmp("cv", folds = 3),
                        measure = msr("regr.mae"),
                        terminator = trm("evals", n_evals = 8))

design_tweedie = benchmark_grid(
  tasks = task_tweedie, 
  learners = list(at_tweedie, lrn_featureless),
  resamplings = rsmp("cv", folds=3)
)

at_tweedie$train(task_tweedie)

results = benchmark(design = design_tweedie)

#results$aggregate(list(msr("regr.mse"),msr("regr.mae")))


#at_tweedie$train(task_tweedie)
#at_tweedie$archive$best()

best_tweedie = as_learner(graph %>>% lrn("regr.xgboost", objective="reg:tweedie", max_depth=2,
                                         nrounds = 348, 
                                         eta = 0.1869081,
                                         tweedie_variance_power = 1.667094))

best_tweedie$train(task_tweedie)


##############################################################################################
                                    ##Interpretable##
##############################################################################################


data_target = data$Cost_claims_year
data_X = data[,-"Cost_claims_year"]

#Impute missing data length
data_X[is.na(get("Length")),("Length"):=mean(data_X[["Length"]],na.rm=TRUE)]

#Impute missing data DC,TF
data_X[is.na(get("Distribution_channel")), 
       ("Distribution_channel") := sample(data_X[!is.na(get("Distribution_channel")), get("Distribution_channel")],
                                          .N, replace = TRUE)]
data_X[is.na(get("Type_fuel")), 
       ("Type_fuel") := sample(data_X[!is.na(get("Type_fuel")), get("Type_fuel")],
                               .N, replace = TRUE)]

data_X$Payment = as.numeric(data_X$Payment)
data_X$Distribution_channel = as.numeric(data_X$Distribution_channel)
data_X$Area = as.numeric(data_X$Area)
data_X$Second_driver = as.numeric(data_X$Second_driver)
data_X$Type_fuel = as.numeric(ifelse(data_X$Type_fuel == "P", 1, ifelse(data_X$Type_fuel == "D", 0, data_X$Type_fuel)))

xg <- xgboost(data = data_matrix, label = data_target ,
              params = list(max_depth = 2, eta = .01869081, 
                            objective = "reg:tweedie", tweedie_variance_power = 1.667094), nrounds = 348,
              verbose = 0)

data_matrix = model.matrix(~.-1, data= data_X)


res <- glex(xg, data_matrix) #### calculates functional decomposition with marginal identification
res$m

theme_set(theme_minimal(base_size = 13))
vi_xgb <- glex_vi(res) 

p_vi <- autoplot(vi_xgb, threshold = 0.02) + 
  labs(title = NULL, tag = "XGBoost-explanation")

p_vi+
  plot_annotation(title = "Variable importance scores by term") & 
  theme(plot.tag.position = "bottomleft")


Payment1_value <- data_matrix[,"Payment1"]

barplot(as.numeric(c(res$m$ Payment1[which.min(Payment1_value==0)], res$m$Payment1[which.min(Payment1_value==1)])),
        names.arg=c("Half-year,Annual"))


xgboost_exp = explain_mlr3(best_tweedie,
                           data = data[,-"Cost_claims_year"],
                           y = data_target,
                           label = "Claims",
                           colorize = FALSE)

random_pct = data[7361,-"Cost_claims_year"]
random_pct

predict(xgboost_exp, random_pct)
plot(predict_parts(xgboost_exp, new_observation = random_pct))
plot(predict_parts(xgboost_exp, new_observation = random_pct, type="shap"))

res$m

#ICE
random_rows_30 <- sample(1:nrow(data), 30)

plot(predict_profile(xgboost_exp, data[random_rows_30,-"Cost_claims_year"]))

#PDP
xg_profiles = model_profile(xgboost_exp)

plot(xg_profiles) +
  theme(legend.position = "top") +
  ggtitle("Partial Dependence for insurance data","")
